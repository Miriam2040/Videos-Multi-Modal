{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70758354-3bb4-42ec-9a9e-7cbc8481e851",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28539ae-5ed3-4901-b817-37f28157b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f14536-4985-437e-bc43-e08574a043e1",
   "metadata": {},
   "source": [
    "# Video Question Answering System using LLaVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b8c83-df2c-4911-a834-c77d919d28d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our VideoQASystem leverages [LLaVA](https://llava-vl.github.io/), a state-of-the-art open-source multimodal model that combines vision and language understanding. While LLaVA was originally designed for single-image multimodal interactions, we extend its capabilities to video analysis through sequential frame processing. The system demonstrates how open-source multimodal AI can understand both visual content and natural language queries to enable intelligent video analysis.\n",
    "\n",
    "**System Architecture**\n",
    "\n",
    "1. **Video Processing**:\n",
    "  - Extracts key frames uniformly from video (4 frames by default)\n",
    "  - Processes each frame independently through LLaVA's multimodal encoder\n",
    "  - Combines insights from multiple frames for temporal understanding\n",
    "\n",
    "2. **Question Answering Pipeline**:\n",
    "  - Takes free-form questions about video content\n",
    "  - Analyzes each frame with LLaVA's multimodal capabilities\n",
    "  - Synthesizes observations into coherent answers using Llama-2 LLM\n",
    "  - Displays video alongside answers for reference\n",
    "\n",
    "3. **Question Generation**:\n",
    "  - Generates relevant questions from video descriptions\n",
    "  - Creates diverse questions focusing on visible content\n",
    "  - Automatically answers generated questions using multimodal analysis\n",
    "  - Shows both video and original description\n",
    "\n",
    "**Implementation Details**\n",
    "- Uses open-source Ollama API to access LLaVA's multimodal capabilities\n",
    "- Converts frames to base64 for API compatibility\n",
    "- Handles both direct questions and auto-generated questions\n",
    "- Provides comprehensive multimodal analysis with video display\n",
    "\n",
    "**Key Features**\n",
    "- Zero-shot multimodal understanding\n",
    "- Multi-frame temporal analysis\n",
    "- Natural language interaction\n",
    "- Visual context preservation\n",
    "- Automated question generation and answering\n",
    "\n",
    "The system leverages open-source tools and LLaVA's powerful multimodal capabilities to bridge the gap between visual understanding and natural language processing, enabling sophisticated video analysis through sequential frame processing and temporal context integration. This open-source implementation makes advanced video understanding accessible and customizable for various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75641a0b-0d0f-46dc-af05-00897dbca61c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VideoQASystem:\n",
    "    def __init__(self, df, video_base_path=\"test_1k_compress\"):\n",
    "        self.df = df\n",
    "        self.video_base_path = video_base_path\n",
    "\n",
    "    def extract_frames(self, video_id: str, num_frames: int = 4):\n",
    "        \"\"\"Extract frames from video\"\"\"\n",
    "        video_path = f\"{self.video_base_path}/{video_id}.mp4\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frames_to_sample = np.linspace(0, total_frames-1, num_frames, dtype=int)\n",
    "        \n",
    "        frames = []\n",
    "        for frame_idx in frames_to_sample:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def frame_to_base64(self, frame):\n",
    "        \"\"\"Convert numpy array frame to base64 string\"\"\"\n",
    "        success, buffer = cv2.imencode('.jpg', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        if not success:\n",
    "            raise ValueError(\"Could not encode image\")\n",
    "        return base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "    def display_video(self, video_id: str):\n",
    "        \"\"\"Display video and its description\"\"\"\n",
    "        video_desc = self.df[self.df['video_id'] == video_id]['sentence'].iloc[0]\n",
    "        video_path = f\"{self.video_base_path}/{video_id}.mp4\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"üìΩÔ∏è Video Analysis: {video_id}\")\n",
    "        print(f\"üìù Description: {video_desc}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        display(HTML(f\"\"\"\n",
    "        <video width=\"500\" controls>\n",
    "            <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "            Your browser does not support the video tag.\n",
    "        </video>\n",
    "        \"\"\"))\n",
    "\n",
    "    def answer_question(self, video_id: str, question: str):\n",
    "        \"\"\"Analyze video frames sequentially with LLaVA\"\"\"\n",
    "        try:\n",
    "            # Display video and description first\n",
    "            self.display_video(video_id)\n",
    "\n",
    "            # Get video description\n",
    "            video_desc = self.df[self.df['video_id'] == video_id]['sentence'].iloc[0]\n",
    "\n",
    "            print(f\"‚ùì Question: {question}\\n\")\n",
    "            print(\"üîÑ Analyzing frames...\")\n",
    "\n",
    "            frames = self.extract_frames(video_id, num_frames=4)\n",
    "\n",
    "            # Analyze each frame\n",
    "            frame_insights = []\n",
    "            for i, frame in enumerate(frames, 1):\n",
    "                print(f\"Processing frame {i}/{len(frames)}...\")\n",
    "                frame_prompt = f\"\"\"Frame {i}/{len(frames)}\n",
    "                Question: {question}\n",
    "                Describe what you see in this frame that helps answer the question.\"\"\"\n",
    "\n",
    "                response = requests.post(\n",
    "                    'http://opmlgpubuild01:11434/api/generate',\n",
    "                    json={\n",
    "                        'model': 'llava',\n",
    "                        'prompt': frame_prompt,\n",
    "                        'images': [self.frame_to_base64(frame)],\n",
    "                        'stream': False\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                frame_insight = response.json().get('response', '').strip()\n",
    "                if frame_insight:\n",
    "                    frame_insights.append(frame_insight)\n",
    "\n",
    "            # Synthesize final answer\n",
    "            if frame_insights:\n",
    "                synthesis_prompt = f\"\"\"Based on these observations:\n",
    "                {' '.join(frame_insights)}\n",
    "\n",
    "                Provide a clear and concise answer to: {question}\"\"\"\n",
    "\n",
    "                final_response = requests.post(\n",
    "                    'http://opmlgpubuild01:11434/api/generate',\n",
    "                    json={\n",
    "                        'model': 'llama3',\n",
    "                        'prompt': synthesis_prompt,\n",
    "                        'stream': False\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                answer = final_response.json().get('response', '').strip()\n",
    "                print(f\"\\nüí° Answer: {answer}\")\n",
    "                return answer\n",
    "            else:\n",
    "                return \"Could not analyze video frames.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_video_with_questions(self, video_id: str):\n",
    "        \"\"\"Complete video analysis with auto-generated questions and answers\"\"\"\n",
    "        # First display the video\n",
    "        self.display_video(video_id)\n",
    "        \n",
    "        # Get video description\n",
    "        video_desc = self.df[self.df['video_id'] == video_id]['sentence'].iloc[0]\n",
    "        \n",
    "        # Generate questions from description\n",
    "        print(\"ü§î Generating relevant questions...\")\n",
    "        prompt = f\"\"\"Given this video description: \"{video_desc}\"\n",
    "        Generate 5 specific and relevant questions about what might be shown in the video.\n",
    "        Questions should focus on visible actions, objects, and details.\n",
    "        Make questions diverse and interesting.\n",
    "        Return only the questions, one per line.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                'http://opmlgpubuild01:11434/api/generate',\n",
    "                json={\n",
    "                    'model': 'llama3',\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            questions = [q.strip().lstrip('123456789.)- ') \n",
    "                        for q in response.json()['response'].strip().split('\\n')\n",
    "                        if '?' in q][:5]\n",
    "            \n",
    "            # Answer each question\n",
    "            print(\"\\nüìã Questions & Answers:\")\n",
    "            for i, question in enumerate(questions, 1):\n",
    "                print(f\"\\n{'-'*50}\")\n",
    "                print(f\"Q{i}: {question}\")\n",
    "                print(f\"A: {self.answer_question(video_id, question)}\")\n",
    "            \n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in analysis: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b90a771-fad9-4d3b-b75b-5af22a481493",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19eaba-031b-46a6-b01e-99833442aadd",
   "metadata": {},
   "source": [
    "We will use the 1K [MSR-VTT dataset](https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/), a subset of the Microsoft Research Video to Text dataset designed for open-domain video captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54dd7c6-4d7f-4f15-9faf-eead6e0c7c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  210M  100  210M    0     0  2877k      0  0:01:14  0:01:14 --:--:-- 2906k\n"
     ]
    }
   ],
   "source": [
    "! curl -L https://github.com/towhee-io/examples/releases/download/data/text_video_search.zip -O\n",
    "! unzip -q -o text_video_search.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7094bc-62c1-4876-99b5-8a99b27f220a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video9770</td>\n",
       "      <td>a person is connecting something to system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video9771</td>\n",
       "      <td>a little girl does gymnastics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video7020</td>\n",
       "      <td>a woman creating a fondant baby and flower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video9773</td>\n",
       "      <td>a boy plays grand theft auto 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video7026</td>\n",
       "      <td>a man is giving a review on a vehicle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    video_id                                    sentence\n",
       "0  video9770  a person is connecting something to system\n",
       "1  video9771               a little girl does gymnastics\n",
       "2  video7020  a woman creating a fondant baby and flower\n",
       "3  video9773              a boy plays grand theft auto 5\n",
       "4  video7026       a man is giving a review on a vehicle"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('MSRVTT_JSFUSION_test.csv')[['video_id', 'sentence']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f80783-d917-4627-bc05-58cd6f82a477",
   "metadata": {},
   "source": [
    "# Video Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b8ba6-2ce4-4b21-938d-179db7e122ab",
   "metadata": {},
   "source": [
    "The answer_question function takes a video ID and a question as input. It first displays the video and its original description. Then it extracts 4 key frames from the video, which are processed sequentially through LLaVA, a multimodal AI model. For each frame, LLaVA analyzes the visual content in context of the question and provides insights. Finally, these frame-by-frame insights are combined to generate a comprehensive answer that considers information from all parts of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04da8b1-d27d-495e-a04a-6ff6753a2d90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìΩÔ∏è Video Analysis: video8869\n",
      "üìù Description: a girl shows a pack of toy building blocks\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"500\" controls>\n",
       "            <source src=\"test_1k_compress/video8869.mp4\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What are the main activities happening in this video?\n",
      "\n",
      "üîÑ Analyzing frames...\n",
      "Processing frame 1/4...\n",
      "Processing frame 2/4...\n",
      "Processing frame 3/4...\n",
      "Processing frame 4/4...\n",
      "\n",
      "üí° Answer: Based on the observations, the main activities happening in this video appear to be:\n",
      "\n",
      "1. Unpacking or opening a package containing small items, such as Lego bricks.\n",
      "2. Handling and assembling Lego bricks to create something, possibly working on a small project or playing with the Lego set.\n",
      "3. Using special connectors and decorative elements to build and configure the Lego structure.\n",
      "\n",
      "These activities suggest that the video is likely focused on creative play or building with Lego sets, and may be part of a larger activity or project.\n",
      "\n",
      "Q: What are the main activities happening in this video?\n",
      "A: Based on the observations, the main activities happening in this video appear to be:\n",
      "\n",
      "1. Unpacking or opening a package containing small items, such as Lego bricks.\n",
      "2. Handling and assembling Lego bricks to create something, possibly working on a small project or playing with the Lego set.\n",
      "3. Using special connectors and decorative elements to build and configure the Lego structure.\n",
      "\n",
      "These activities suggest that the video is likely focused on creative play or building with Lego sets, and may be part of a larger activity or project.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the system\n",
    "video_qa = VideoQASystem(data)\n",
    "\n",
    "# Test with the same video\n",
    "video_id = \"video8869\"\n",
    "question = \"What are the main activities happening in this video?\"\n",
    "\n",
    "# Get answer\n",
    "answer = video_qa.answer_question(video_id, question)\n",
    "print(f\"\\nQ: {question}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c5276-6b1c-4a1b-86c1-2306edae1487",
   "metadata": {},
   "source": [
    "## Automatic Video Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756f0bb-3e66-4567-a31c-1d4294b6dab7",
   "metadata": {},
   "source": [
    "The analyze_video_with_questions function automatically processes a video in two steps: First, it uses the video's text description to generate relevant questions using an LLM (Llama-3). Then, for each generated question, it performs visual analysis using LLaVA to provide answers, displaying the video and its description alongside the Q&A results. This creates a comprehensive analysis of the video content through automatically generated and answered questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaeffa56-3235-4525-9cb1-02cc8848e9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìΩÔ∏è Video Analysis: video8869\n",
      "üìù Description: a girl shows a pack of toy building blocks\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"500\" controls>\n",
       "            <source src=\"test_1k_compress/video8869.mp4\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§î Generating relevant questions...\n",
      "\n",
      "üìã Questions & Answers:\n",
      "\n",
      "--------------------------------------------------\n",
      "Q1: Are the toy building blocks being built into a specific structure or shape?\n",
      "\n",
      "==================================================\n",
      "üìΩÔ∏è Video Analysis: video8869\n",
      "üìù Description: a girl shows a pack of toy building blocks\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"500\" controls>\n",
       "            <source src=\"test_1k_compress/video8869.mp4\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Are the toy building blocks being built into a specific structure or shape?\n",
      "\n",
      "üîÑ Analyzing frames...\n",
      "Processing frame 1/4...\n",
      "Processing frame 2/4...\n",
      "Processing frame 3/4...\n",
      "Processing frame 4/4...\n",
      "\n",
      "üí° Answer: Based on the observations, it appears that the person is likely building according to a design plan, possibly following instructions from an included LEGO instruction manual. While the specific shape or structure being built is not clearly visible due to the angle of the photo and the positioning of the person's hands and the blocks, the fact that the person is carefully placing a block suggests that they are intentionally building a specific design rather than simply playing with the blocks.\n",
      "A: Based on the observations, it appears that the person is likely building according to a design plan, possibly following instructions from an included LEGO instruction manual. While the specific shape or structure being built is not clearly visible due to the angle of the photo and the positioning of the person's hands and the blocks, the fact that the person is carefully placing a block suggests that they are intentionally building a specific design rather than simply playing with the blocks.\n",
      "\n",
      "--------------------------------------------------\n",
      "Q2: Can you identify any distinctive colors or patterns on the blocks themselves?\n",
      "\n",
      "==================================================\n",
      "üìΩÔ∏è Video Analysis: video8869\n",
      "üìù Description: a girl shows a pack of toy building blocks\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"500\" controls>\n",
       "            <source src=\"test_1k_compress/video8869.mp4\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Can you identify any distinctive colors or patterns on the blocks themselves?\n",
      "\n",
      "üîÑ Analyzing frames...\n",
      "Processing frame 1/4...\n",
      "Processing frame 2/4...\n",
      "Processing frame 3/4...\n",
      "\n",
      "üí° Answer: Yes, based on the observations, some distinctive colors and patterns that can be identified on the LEGO blocks include:\n",
      "\n",
      "* Solid colors such as yellow, red, blue, white, purple, green, and possibly black\n",
      "* Patterns such as stripes, dots, textures, and geometric shapes like rectangles and circles\n",
      "* Additional decorations or stickers on some blocks adding to their visual diversity\n",
      "* Various shades of these colors, including different hues of yellow, green, red, and blue\n",
      "\n",
      "These colors and patterns suggest a wide range of options for creating different scenes or structures with the LEGO blocks.\n",
      "A: Yes, based on the observations, some distinctive colors and patterns that can be identified on the LEGO blocks include:\n",
      "\n",
      "* Solid colors such as yellow, red, blue, white, purple, green, and possibly black\n",
      "* Patterns such as stripes, dots, textures, and geometric shapes like rectangles and circles\n",
      "* Additional decorations or stickers on some blocks adding to their visual diversity\n",
      "* Various shades of these colors, including different hues of yellow, green, red, and blue\n",
      "\n",
      "These colors and patterns suggest a wide range of options for creating different scenes or structures with the LEGO blocks.\n",
      "\n",
      "--------------------------------------------------\n",
      "Q3: Does the girl demonstrate any unique techniques for stacking or connecting the blocks?\n",
      "\n",
      "==================================================\n",
      "üìΩÔ∏è Video Analysis: video8869\n",
      "üìù Description: a girl shows a pack of toy building blocks\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"500\" controls>\n",
       "            <source src=\"test_1k_compress/video8869.mp4\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Does the girl demonstrate any unique techniques for stacking or connecting the blocks?\n",
      "\n",
      "üîÑ Analyzing frames...\n",
      "Processing frame 1/4...\n",
      "Processing frame 2/4...\n",
      "Processing frame 3/4...\n",
      "Processing frame 4/4...\n",
      "\n",
      "üí° Answer: Based on the provided observation, it is difficult to determine if the girl demonstrates any unique techniques for stacking or connecting the blocks without more context or specific knowledge about the type of blocks and how they are intended to fit together. The image shows a person's hands interacting with Lego blocks in a typical manner, suggesting that she may be using standard manual assembly methods rather than any extraordinary or innovative approaches.\n",
      "A: Based on the provided observation, it is difficult to determine if the girl demonstrates any unique techniques for stacking or connecting the blocks without more context or specific knowledge about the type of blocks and how they are intended to fit together. The image shows a person's hands interacting with Lego blocks in a typical manner, suggesting that she may be using standard manual assembly methods rather than any extraordinary or innovative approaches.\n",
      "\n",
      "--------------------------------------------------\n",
      "Q4: Are there any other toys or objects visible in the background that might be relevant to the building activity?\n",
      "\n",
      "==================================================\n",
      "üìΩÔ∏è Video Analysis: video8869\n",
      "üìù Description: a girl shows a pack of toy building blocks\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"500\" controls>\n",
       "            <source src=\"test_1k_compress/video8869.mp4\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Are there any other toys or objects visible in the background that might be relevant to the building activity?\n",
      "\n",
      "üîÑ Analyzing frames...\n",
      "Processing frame 1/4...\n",
      "Processing frame 2/4...\n",
      "Processing frame 3/4...\n",
      "Processing frame 4/4...\n",
      "\n",
      "üí° Answer: Yes, there are several small Lego-like bricks scattered around on the table, suggesting that the person is either organizing or sorting them for a building project. Additionally, there appears to be a small toy car among the toys, which may also be related to the construction activity, as it could serve as part of the structure being built or simply as a decoration or additional piece to add to the Lego creation.\n",
      "A: Yes, there are several small Lego-like bricks scattered around on the table, suggesting that the person is either organizing or sorting them for a building project. Additionally, there appears to be a small toy car among the toys, which may also be related to the construction activity, as it could serve as part of the structure being built or simply as a decoration or additional piece to add to the Lego creation.\n",
      "\n",
      "--------------------------------------------------\n",
      "Q5: Do the blocks appear to have different shapes or textures, such as spheres, cylinders, or flat pieces?\n",
      "\n",
      "==================================================\n",
      "üìΩÔ∏è Video Analysis: video8869\n",
      "üìù Description: a girl shows a pack of toy building blocks\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"500\" controls>\n",
       "            <source src=\"test_1k_compress/video8869.mp4\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Do the blocks appear to have different shapes or textures, such as spheres, cylinders, or flat pieces?\n",
      "\n",
      "üîÑ Analyzing frames...\n",
      "Processing frame 1/4...\n",
      "Processing frame 2/4...\n",
      "Processing frame 3/4...\n",
      "Processing frame 4/4...\n",
      "\n",
      "üí° Answer: Yes, the blocks appear to have different shapes and textures. The image shows cylindrical pieces (standard LEGO bricks), flat plates, and rectangular bricks with varying colors and designs. There may also be spherical or other non-rectangular pieces present, although it is difficult to determine their exact shape due to the low resolution of the photo.\n",
      "A: Yes, the blocks appear to have different shapes and textures. The image shows cylindrical pieces (standard LEGO bricks), flat plates, and rectangular bricks with varying colors and designs. There may also be spherical or other non-rectangular pieces present, although it is difficult to determine their exact shape due to the low resolution of the photo.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Analyze a video with auto-generated questions\n",
    "video_id = \"video8869\"\n",
    "video_qa.analyze_video_with_questions(video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33a317-2aab-4894-9500-20e92a9896a2",
   "metadata": {},
   "source": [
    "# Additional Thoughts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac99ae-3426-4119-8c2e-de35faea4315",
   "metadata": {},
   "source": [
    "**Additional Enhancements for Video Q&A System:**\n",
    "\n",
    "**Advanced Model Integration**\n",
    "\n",
    "- Direct integration with [MiniGPT4-video](https://vision-cair.github.io/MiniGPT4-video/):\n",
    "   * Replace frame-by-frame LLaVA analysis with single pass through MiniGPT4-video\n",
    "   * Better temporal understanding and more coherent answers\n",
    "   * Supports longer video context windows\n",
    "\n",
    "- Object and action detection:\n",
    "   * Use DINO-v2 to detect key objects and activities\n",
    "   * Add detection results to prompts (\"I see a person cooking, holding a pan...\")\n",
    "   * Create richer context for more accurate answers\n",
    "\n",
    "- Scene segmentation:\n",
    "   * Detect scene changes in video\n",
    "   * Split questions/answers by relevant scenes\n",
    "   * Enable temporal localization (\"in the first scene...\")\n",
    "\n",
    "**Improved Analysis**\n",
    "\n",
    "- Multi-turn conversations:\n",
    "   * Keep conversation history and video context\n",
    "   * Allow follow-up questions about previous answers\n",
    "   * Enable clarifications and deeper exploration\n",
    "\n",
    "- Timestamp-specific questions:\n",
    "   * Link answers to specific video timestamps\n",
    "   * Allow questions about specific moments\n",
    "   * Navigate to relevant parts of video when answering\n",
    "\n",
    "- Visual highlighting:\n",
    "   * Overlay detected objects/actions on video\n",
    "   * Show relevant regions for each answer\n",
    "   * Help users understand model's focus areas\n",
    "\n",
    "- Cross-video references:\n",
    "   * Compare similar scenes across videos\n",
    "   * Answer questions spanning multiple videos\n",
    "   * Find related content in video database\n",
    "\n",
    "This enhanced system would combine specialized video understanding with rich contextual information from detectors, enabling more precise and interactive video analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
